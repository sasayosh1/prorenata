#!/usr/bin/env python3
"""
Search Console ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

éå»30æ—¥é–“ã®æ¤œç´¢ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€CSVã«ä¿å­˜ã—ã¾ã™ã€‚
- ã‚¯ãƒªãƒƒã‚¯æ•°
- è¡¨ç¤ºå›æ•°
- CTR
- å¹³å‡æ²è¼‰é †ä½
- æ¤œç´¢ã‚¯ã‚¨ãƒªã€ãƒšãƒ¼ã‚¸URLã€å›½ã€ãƒ‡ãƒã‚¤ã‚¹åˆ¥

ç’°å¢ƒå¤‰æ•°:
- GOOGLE_APPLICATION_CREDENTIALS: ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆJSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
- GSC_SITE_URL: Search Consoleã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£URLï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: https://prorenata.jp/ï¼‰
"""

import os
import sys
from datetime import date, timedelta
from googleapiclient.discovery import build
import pandas as pd

def main():
    # è¨­å®š
    SITE_URL = os.environ.get("GSC_SITE_URL", "https://prorenata.jp/")
    OUTPUT_FILE = "data/gsc_last30d.csv"

    # èªè¨¼æƒ…å ±ã®ç¢ºèª
    # èªè¨¼æƒ…å ±ã®ç¢ºèª
    if not os.environ.get("GOOGLE_APPLICATION_CREDENTIALS"):
        print("Warning: GOOGLE_APPLICATION_CREDENTIALS ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        # ã‚¨ãƒ©ãƒ¼ã§ã¯ãªãæ­£å¸¸çµ‚äº†ã¨ã—ã¦æ‰±ã„ã€å¾Œç¶šã®ãƒ—ãƒ­ã‚»ã‚¹ã§ãƒ‡ãƒ¼ã‚¿ãŒãªã„ã“ã¨ã‚’æ¤œçŸ¥ã•ã›ã‚‹
        sys.exit(0)

    print(f"ğŸ” Search Console ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹")
    print(f"   ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£: {SITE_URL}")

    try:
        # Search Console API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆ
        service = build("searchconsole", "v1")

        # æœŸé–“è¨­å®šï¼ˆéå»30æ—¥é–“ï¼‰
        end_date = date.today()
        start_date = end_date - timedelta(days=30)

        print(f"   æœŸé–“: {start_date} ã€œ {end_date}")

        # ã‚¯ã‚¨ãƒªãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        request_body = {
            "startDate": start_date.isoformat(),
            "endDate": end_date.isoformat(),
            "dimensions": ["date", "page", "query", "country", "device"],
            "rowLimit": 25000,
            "dataState": "final"  # ç¢ºå®šãƒ‡ãƒ¼ã‚¿ã®ã¿
        }

        # APIå®Ÿè¡Œ
        response = service.searchanalytics().query(
            siteUrl=SITE_URL,
            body=request_body
        ).execute()

        # ãƒ‡ãƒ¼ã‚¿æ•´å½¢
        rows = response.get("rows", [])
        data = []

        for row in rows:
            keys = row.get("keys", [])
            data.append({
                "date": keys[0] if len(keys) > 0 else "",
                "page": keys[1] if len(keys) > 1 else "",
                "query": keys[2] if len(keys) > 2 else "",
                "country": keys[3] if len(keys) > 3 else "",
                "device": keys[4] if len(keys) > 4 else "",
                "clicks": row.get("clicks", 0),
                "impressions": row.get("impressions", 0),
                "ctr": row.get("ctr", 0.0),
                "position": row.get("position", 0.0),
            })

        # CSVä¿å­˜
        df = pd.DataFrame(data)
        os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
        df.to_csv(OUTPUT_FILE, index=False, encoding="utf-8")

        print(f"âœ… ä¿å­˜å®Œäº†: {OUTPUT_FILE}")
        print(f"   ãƒ‡ãƒ¼ã‚¿è¡Œæ•°: {len(df):,}")
        print(f"   ç·ã‚¯ãƒªãƒƒã‚¯æ•°: {df['clicks'].sum():,}")
        print(f"   ç·è¡¨ç¤ºå›æ•°: {df['impressions'].sum():,}")
        print(f"   å¹³å‡CTR: {df['ctr'].mean():.2%}")
        print(f"   å¹³å‡æ²è¼‰é †ä½: {df['position'].mean():.1f}")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
